{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "LASER_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "pycharm-64540134",
   "language": "python",
   "display_name": "PyCharm (science4carrots)"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import laserembeddings\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import langdetect\n",
    "from src.Util import *\n",
    "import copy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "!python -m laserembeddings download-models\n",
    "laser=laserembeddings.Laser()\n",
    "\n",
    "# set accordingly\n",
    "DEV_MODE = True\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('Using CPU.')\n",
    "    device = torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X23hp7aO7yLr"
   },
   "source": [
    "def determine_tweet_lang(x):\n",
    "    try:\n",
    "      return_val = langdetect.detect(x)\n",
    "    except langdetect.lang_detect_exception.LangDetectException:\n",
    "      # language not detectable from text (e.g., just numbers given, etc.)\n",
    "      # the exact language is not important (just used for tokenization), we can assume it was English\n",
    "      return_val='en'\n",
    "    except:\n",
    "      #catches any kind of error that makes it impossible to detect language\n",
    "      return_val=None\n",
    "\n",
    "    return return_val\n",
    "\n",
    "def prepare_LASER_dataset(data, inplace=True):\n",
    "  tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "  transform_labels(data)\n",
    "\n",
    "  decoded_tweet_text=data['text_tokens']\\\n",
    "      .apply(lambda text_tokens: \"\".join(tokenizer.\n",
    "                                         convert_tokens_to_string(\n",
    "      tokenizer.convert_ids_to_tokens(text_tokens.split('\\t')))).replace('[CLS]','').replace('[SEP]',''))\\\n",
    "      .astype(str)\n",
    "\n",
    "  lang=decoded_tweet_text.apply(lambda x: determine_tweet_lang(x))\\\n",
    "      .apply(lambda x: x.replace('-cn','').replace('-tw','')).astype(str)\n",
    "\n",
    "\n",
    "  laser_embedding=pd.concat([decoded_tweet_text, lang], axis =1)\\\n",
    "      .apply(lambda row:laser.embed_sentences(row[0],row[1]),axis=1)\\\n",
    "      .apply(lambda x:x.flatten())\n",
    "\n",
    "  if not inplace:\n",
    "    data = copy.deepcopy(data)\n",
    "\n",
    "  data['laser_embedding']=laser_embedding\n",
    "  data['lang']=lang\n",
    "\n",
    "  return data\n",
    "\n",
    "def scatterplot_tsne(tsne_embeddings, target_labels, title):\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  plt.title(title)\n",
    "  colors = ['r', 'g']\n",
    "  for i in range(len(tsne_embeddings)):\n",
    "      plt.scatter(tsne_embeddings[i, 0], tsne_embeddings[i, 1], c=colors[target_labels[i]])\n",
    "  plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Aw28qy4hTyhF",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "outputId": "8ed3681c-a189-4aae-f7a1-14851995a3ab"
   },
   "source": [
    "train_path=''\n",
    "test_path=''\n",
    "print('Reading data.')\n",
    "train_data = pd.read_csv(train_path,sep='\\x01',encoding = 'utf-8',names=CONTENT_BASED_COLUMNS+LABELS,\n",
    "                         header=None,usecols=CONTENT_BASED_COLUMNS + LABELS)\n",
    "test_data = pd.read_csv(test_path,sep='\\x01',encoding = 'utf-8',names=CONTENT_BASED_COLUMNS+LABELS,\n",
    "                        header=None,usecols= CONTENT_BASED_COLUMNS + LABELS)\n",
    "\n",
    "if DEV_MODE:\n",
    "  print('Using reduced samples.')\n",
    "  train_data=train_data.sample(10000)\n",
    "  test_data=test_data.sample(10000)\n",
    "\n",
    "print('Preparing train and test data.')\n",
    "train_data=prepare_LASER_dataset(train_data)\n",
    "print('Done preparing train data.')\n",
    "test_data=prepare_LASER_dataset(test_data)\n",
    "print('Done')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Laser (https://github.com/facebookresearch/LASER) is short for language agnostic sentence representations and can be used to generate embeddings for entire phrases that conserve semantic meaning across different languages.\n",
    "In the below example, the different statements about food preferences are at a small distance to each other in vector space, thus representing their semantic similarity. The other phrase, expressing a preference over animals instead, is much further away."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-PBRz9Lge4Tt",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "outputId": "a797b246-a036-47ad-9058-374ed9412be4"
   },
   "source": [
    "sentences= ['I love pasta.',\"J'adore les p√¢tes.\",'Ich liebe Pasta.','Amo le paste.','Dogs are better than cats.']\n",
    "langs=['en', 'fr', 'de','it','en'] \n",
    "embeddings = laser.embed_sentences(sentences,langs)\n",
    "print(f'The shape of the embeddings {embeddings.shape}')\n",
    "\n",
    "#illustrates langdetect\n",
    "detected_languages=[]\n",
    "for sentence in sentences:\n",
    "    # Note that even though language detection on short phrases is far from optimal, this does not matter as LASER only\n",
    "    # needs the information for tokenization (i.e., language AGNOSTIC) and tokenization should not be overly affected.\n",
    "    detected_languages.append(langdetect.detect(sentence))\n",
    "dist = [[ np.linalg.norm(a-b) for b in embeddings] for a in embeddings]\n",
    "\n",
    "index = sentences\n",
    "columns = sentences\n",
    "df = pd.DataFrame(dist, index=index, columns=columns)\n",
    "plt.pcolor(df)\n",
    "plt.yticks(np.arange(0.5, len(df.index), 1), df.index)\n",
    "plt.xticks(np.arange(0.5, len(df.columns), 1), df.columns,rotation=90)\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMImVbpDOv7p"
   },
   "source": [
    "We now embed the different Tweets using LASER and build algorithms to classify engagement probabilities on them.\n",
    "The intuition is that these algorithms will be able to use the overall semantic meaning encoded in the embeddings to detect which Tweets are more likley to foster certain types of interaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WO-JiLHSj17v"
   },
   "source": [
    "knn_grid = {'n_neighbors':[10, 100, 1000]}\n",
    "\n",
    "for label in LABELS:\n",
    "  print(f'Current label: {label}')\n",
    "  for params in ParameterGrid(knn_grid):\n",
    "    print(params)\n",
    "    knn_cls = KNeighborsClassifier(n_neighbors=params['n_neighbors'])\n",
    "    knn_cls.fit(train_data['laser_embedding'].tolist(),train_data[label].tolist())\n",
    "    predictions = knn_cls.predict(test_data['laser_embedding'].tolist())\n",
    "    compute_all_metrics(predictions, test_data[label].tolist())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Uu5q78Yp5Lki"
   },
   "source": [
    "laser_embeddings=torch.tensor(train_data['laser_embedding'].tolist())\n",
    "labels=torch.tensor(train_data[LABELS].values.tolist())\n",
    "train_dataset=TensorDataset(laser_embeddings, labels)\n",
    "laser_embeddings_test=torch.tensor(test_data['laser_embedding'].tolist())\n",
    "labels_test=torch.tensor(test_data[LABELS].values.tolist())\n",
    "test_dataset=TensorDataset(laser_embeddings_test,labels_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llNPckZdvl9H",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "outputId": "c94a65dc-0856-4b98-a587-448cf7a8b6f4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We build a small neural network to classify on top of these embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,input_size, hidden_size1, hidden_size2,hidden_size3, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        #1024 dimensional inputs\n",
    "        self.fc0   = torch.nn.Linear(input_size,hidden_size1)\n",
    "        self.relu1 = torch.nn.LeakyReLU()\n",
    "        self.fc1   = torch.nn.Linear(hidden_size1,hidden_size2)  \n",
    "        self.relu2 = torch.nn.LeakyReLU()\n",
    "        self.fc2   = torch.nn.Linear(hidden_size2,hidden_size3)\n",
    "        self.relu3 = torch.nn.LeakyReLU()\n",
    "        self.fc3   = torch.nn.Linear(hidden_size3,output_size)\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(self.relu1(self.fc0(x)))\n",
    "        x = self.fc2(self.relu2(x))\n",
    "        x = self.fc3(self.relu3(x))\n",
    "        x = self.sig(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net(1024,256,64,16,4)\n",
    "model.to(device)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will see that the model merely learns the average probability for each class, thus effectively reaching RCE scores of 0 as it matches the naive baseline."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.train()\n",
    "epoch = 20\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print('Training started.')\n",
    "for epoch in range(epoch): \n",
    "  for step, batch in enumerate(train_loader):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    laser_embedding_tensor=batch[0].to(device).float()\n",
    "    label_tensor=batch[1].to(device).float()\n",
    "    y_pred = model(laser_embedding_tensor)   # Compute Loss\n",
    "    loss = criterion(y_pred.squeeze(), label_tensor)\n",
    "   \n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  print('Epoch {}: train loss: {}'.format(epoch, loss.item()))    # Backward pass\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "#performing test run\n",
    "\n",
    "total_predictions=[]\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "\n",
    "    #puts the optimizer into evaluation mode as we are not training it anymore\n",
    "    model.eval()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    laser_embedding_tensor=batch[0].to(device).float()\n",
    "    label_tensor=batch[1].to(device).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      y_pred = model(laser_embedding_tensor)   # Compute Loss\n",
    "    total_predictions=total_predictions+[y_pred]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvlSwFC6f8et",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "66b40653-f617-48bf-9f09-cab919abb39b",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since the obtained results with different methods do not look promising, we generate T-SNE embeddings of the datapoints with respect to their class hoping to determine a relationship between class membership and embedding vector. We use varying levels of perplexity, as recommended. There is no clear relationship between embeddings and class membership visible in the visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# randomly sample 1000 datapoints and generate T-SNE visualizations\n",
    "tsne_data=train_data.sample(1000)\n",
    "for label in LABELS:\n",
    "    for i in range(0,51, 10):\n",
    "      tsne=TSNE(n_components=2,perplexity=i,random_state=0)\n",
    "      transformed_train_embed=tsne.fit_transform(np.array(tsne_data['laser_embedding'].tolist()))\n",
    "      scatterplot_tsne(transformed_train_embed,tsne_data[label].to_numpy(), title=f'Label {label} Perplexity {i}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}